from llama_cpp import Llama

def load_llm():
    try:
        print("ğŸš€ llama.cpp ëª¨ë¸ ë¡œë”© ì‹œì‘")

        llm = Llama(
            model_path="models/llama-3.2-Korean-Bllossom-3B-Q6_K_L.gguf",
            n_ctx=2048,
            n_threads=4,
            verbose=False
        )

        print("âœ… llama.cpp ëª¨ë¸ ë¡œë”© ì™„ë£Œ")
        return llm  # ì´ì œ í•˜ë‚˜ë§Œ ë°˜í™˜í•¨

    except Exception as e:
        import traceback
        print("âŒ LLM ë¡œë”© ì¤‘ ì˜ˆì™¸ ë°œìƒ!")
        traceback.print_exc()
        raise RuntimeError(f"LLM ë¡œë”© ì‹¤íŒ¨: {e}")

