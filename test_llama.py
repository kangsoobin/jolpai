from llama_cpp import Llama

llm = Llama(
    model_path="models/llama-3.2-Korean-Bllossom-3B-Q6_K_L.gguf",
    n_ctx=2048,           # context window
    n_threads=4,          # Macì—ì„œ 4~8 ì¶”ì²œ
    verbose=True
)

prompt = """<|begin_of_text|><|start_header_id|>system<|end_header_id|>
Cutting Knowledge Date: December 2023
Today Date: 29 Oct 2024

ë„ˆëŠ” ì´ì œë¶€í„° ë§¤ìš° ì¹œì ˆí•œ í•œêµ­ì–´ ë„ìš°ë¯¸ì•¼. ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ìì„¸í•˜ê³  ë”°ëœ»í•˜ê²Œ ëŒ€ë‹µí•´ì¤˜.<|eot_id|><|start_header_id|>user<|end_header_id|>
ì„œìš¸ì—ì„œ ê°€ì„ì— ê°€ê¸° ì¢‹ì€ ì—¬í–‰ì§€ ì•Œë ¤ì¤˜<|eot_id|><|start_header_id|>assistant<|end_header_id|>"""

output = llm(prompt, max_tokens=200, stop=["<|eot_id|>"], echo=False)

print("\nğŸ’¬ ëª¨ë¸ ì‘ë‹µ:\n")
print(output["choices"][0]["text"])
